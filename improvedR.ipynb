{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Base URL pointing to the GitHub repository\n",
    "base_url = \"https://raw.githubusercontent.com/Catalan-AI-Studio/Project1/main/data/\"\n",
    "\n",
    "# ERCOT 15-Minute Fuel Mix Generation (megawatt)(2020Q1-2024Q2)\n",
    "\n",
    "# 2024 Data\n",
    "FuelMix2024Q1 = f'{base_url}ercot_gen_all_15min_2024Q1.csv'\n",
    "FuelMixlable2024Q1 = pd.read_csv(FuelMix2024Q1, skiprows=3)\n",
    "\n",
    "FuelMix2024Q2 = f'{base_url}ercot_gen_all_15min_2024Q2.csv'\n",
    "FuelMixlable2024Q2 = pd.read_csv(FuelMix2024Q2, skiprows=3)\n",
    "\n",
    "FuelMix2024Q3 = f'{base_url}ercot_gen_all_15min_2024Q3.csv'\n",
    "FuelMixlable2024Q3 = pd.read_csv(FuelMix2024Q3, skiprows=3)\n",
    "\n",
    "# 2023 Data\n",
    "FuelMix2023Q1 = f'{base_url}ercot_gen_all_15min_2023Q1.csv'\n",
    "FuelMixlable2023Q1 = pd.read_csv(FuelMix2023Q1, skiprows=3)\n",
    "\n",
    "FuelMix2023Q2 = f'{base_url}ercot_gen_all_15min_2023Q2.csv'\n",
    "FuelMixlable2023Q2 = pd.read_csv(FuelMix2023Q2, skiprows=3)\n",
    "\n",
    "FuelMix2023Q3 = f'{base_url}ercot_gen_all_15min_2023Q3.csv'\n",
    "FuelMixlable2023Q3 = pd.read_csv(FuelMix2023Q3, skiprows=3)\n",
    "\n",
    "FuelMix2023Q4 = f'{base_url}ercot_gen_all_15min_2023Q4.csv'\n",
    "FuelMixlable2023Q4 = pd.read_csv(FuelMix2023Q4, skiprows=3)\n",
    "\n",
    "# 2022 Data\n",
    "FuelMix2022Q1 = f'{base_url}ercot_gen_all_15min_2022Q1.csv'\n",
    "FuelMixlable2022Q1 = pd.read_csv(FuelMix2022Q1, skiprows=3)\n",
    "\n",
    "FuelMix2022Q2 = f'{base_url}ercot_gen_all_15min_2022Q2.csv'\n",
    "FuelMixlable2022Q2 = pd.read_csv(FuelMix2022Q2, skiprows=3)\n",
    "\n",
    "FuelMix2022Q3 = f'{base_url}ercot_gen_all_15min_2022Q3.csv'\n",
    "FuelMixlable2022Q3 = pd.read_csv(FuelMix2022Q3, skiprows=3)\n",
    "\n",
    "FuelMix2022Q4 = f'{base_url}ercot_gen_all_15min_2022Q4.csv'\n",
    "FuelMixlable2022Q4 = pd.read_csv(FuelMix2022Q4, skiprows=3)\n",
    "\n",
    "# 2021 Data\n",
    "FuelMix2021Q1 = f'{base_url}ercot_gen_all_15min_2021Q1.csv'\n",
    "FuelMixlable2021Q1 = pd.read_csv(FuelMix2021Q1, skiprows=3)\n",
    "\n",
    "FuelMix2021Q2 = f'{base_url}ercot_gen_all_15min_2021Q2.csv'\n",
    "FuelMixlable2021Q2 = pd.read_csv(FuelMix2021Q2, skiprows=3)\n",
    "\n",
    "FuelMix2021Q3 = f'{base_url}ercot_gen_all_15min_2021Q3.csv'\n",
    "FuelMixlable2021Q3 = pd.read_csv(FuelMix2021Q3, skiprows=3)\n",
    "\n",
    "FuelMix2021Q4 = f'{base_url}ercot_gen_all_15min_2021Q4.csv'\n",
    "FuelMixlable2021Q4 = pd.read_csv(FuelMix2021Q4, skiprows=3)\n",
    "\n",
    "# 2020 Data\n",
    "FuelMix2020Q1 = f'{base_url}ercot_gen_all_15min_2020Q1.csv'\n",
    "FuelMixlable2020Q1 = pd.read_csv(FuelMix2020Q1, skiprows=3)\n",
    "\n",
    "FuelMix2020Q2 = f'{base_url}ercot_gen_all_15min_2020Q2.csv'\n",
    "FuelMixlable2020Q2 = pd.read_csv(FuelMix2020Q2, skiprows=3)\n",
    "\n",
    "FuelMix2020Q3 = f'{base_url}ercot_gen_all_15min_2020Q3.csv'\n",
    "FuelMixlable2020Q3 = pd.read_csv(FuelMix2020Q3, skiprows=3)\n",
    "\n",
    "FuelMix2020Q4 = f'{base_url}ercot_gen_all_15min_2020Q4.csv'\n",
    "FuelMixlable2020Q4 = pd.read_csv(FuelMix2020Q4, skiprows=3)\n",
    "\n",
    "print(FuelMixlable2020Q1.columns)\n",
    "print(FuelMixlable2024Q3.columns)\n",
    "print(FuelMixlable2020Q1.shape)\n",
    "print(FuelMixlable2020Q2.shape)\n",
    "print(FuelMixlable2020Q3.shape)\n",
    "print(FuelMixlable2020Q4.shape)\n",
    "print(FuelMixlable2021Q1.shape)\n",
    "print(FuelMixlable2021Q2.shape)\n",
    "print(FuelMixlable2021Q3.shape)\n",
    "print(FuelMixlable2021Q4.shape)\n",
    "print(FuelMixlable2022Q1.shape)\n",
    "print(FuelMixlable2022Q2.shape)\n",
    "print(FuelMixlable2022Q3.shape)\n",
    "print(FuelMixlable2022Q4.shape)\n",
    "print(FuelMixlable2023Q1.shape)\n",
    "print(FuelMixlable2023Q2.shape)\n",
    "print(FuelMixlable2023Q3.shape)\n",
    "print(FuelMixlable2023Q4.shape)\n",
    "print(FuelMixlable2024Q1.shape)\n",
    "print(FuelMixlable2024Q2.shape)\n",
    "print(FuelMixlable2024Q3.shape)\n",
    "\n",
    "lables = [FuelMixlable2020Q1,FuelMixlable2020Q2,FuelMixlable2020Q3,FuelMixlable2020Q4,FuelMixlable2021Q1,FuelMixlable2021Q2,FuelMixlable2021Q3,FuelMixlable2021Q4,FuelMixlable2022Q1,FuelMixlable2022Q2,FuelMixlable2022Q3,FuelMixlable2022Q4,FuelMixlable2023Q1,FuelMixlable2023Q2,FuelMixlable2023Q3,FuelMixlable2023Q4,FuelMixlable2024Q1,FuelMixlable2024Q2,FuelMixlable2024Q3]\n",
    "FuelMixdata = pd.DataFrame()\n",
    "columns_to_extract = ['UTC Timestamp (Interval Ending)',\n",
    "       'Local Timestamp Central Time (Interval Beginning)',\n",
    "       'Local Timestamp Central Time (Interval Ending)', 'Local Date',\n",
    "       'Hour Number', 'Total Generation (MW)', 'Biomass Generation (MW)',\n",
    "       'Coal Generation (MW)', 'Gas Generation (MW)', 'Gas-CC Generation (MW)',\n",
    "       'Hydro Generation (MW)', 'Nuclear Generation (MW)',\n",
    "       'Other Generation (MW)', 'Solar Generation (MW)',\n",
    "       'Wind Generation (MW)'\n",
    "]\n",
    "for lable in lables:\n",
    "    selected_columns = lable[columns_to_extract]\n",
    "    FuelMixdata = pd.concat([FuelMixdata, selected_columns], ignore_index=True)\n",
    "\n",
    "FuelMixdata.sort_values(by='Local Timestamp Central Time (Interval Ending)', inplace=True)\n",
    "\n",
    "FuelMixdata['Local Timestamp Central Time (Interval Ending)'] = pd.to_datetime(FuelMixdata['Local Timestamp Central Time (Interval Ending)'])\n",
    "\n",
    "FuelMixdata['Local Timestamp Central Time (Interval Ending)'] = FuelMixdata['Local Timestamp Central Time (Interval Ending)'].dt.ceil('H')\n",
    "\n",
    "FuelMix_Hourly_Data = FuelMixdata.groupby('Local Timestamp Central Time (Interval Ending)').agg({\n",
    "    'UTC Timestamp (Interval Ending)': 'first',\n",
    "    'Local Timestamp Central Time (Interval Beginning)': 'first',\n",
    "    'Local Date': 'first',\n",
    "    'Hour Number': 'first',\n",
    "    'Total Generation (MW)': 'sum',\n",
    "    'Biomass Generation (MW)': 'sum',\n",
    "    'Coal Generation (MW)': 'sum',\n",
    "    'Gas Generation (MW)': 'sum',\n",
    "    'Gas-CC Generation (MW)': 'sum',\n",
    "    'Hydro Generation (MW)': 'sum',\n",
    "    'Nuclear Generation (MW)': 'sum',\n",
    "    'Other Generation (MW)': 'sum',\n",
    "    'Solar Generation (MW)': 'sum',\n",
    "    'Wind Generation (MW)': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "FuelMix_Hourly_Data.head()\n",
    "\n",
    "\n",
    "# ERCOT 15-Minute Real-Time Locational Marginal Prices ($/megawatt) for Hubs (2020Q1-2024Q3)\n",
    "\n",
    "# 2024 Data\n",
    "LMP2024Q1 = f'{base_url}ercot_lmp_rt_15min_hubs_2024Q1.csv'\n",
    "RTLMP2024Q1 = pd.read_csv(LMP2024Q1, skiprows=3)\n",
    "\n",
    "LMP2024Q2 = f'{base_url}ercot_lmp_rt_15min_hubs_2024Q2.csv'\n",
    "RTLMP2024Q2 = pd.read_csv(LMP2024Q2, skiprows=3)\n",
    "\n",
    "LMP2024Q3 = f'{base_url}ercot_lmp_rt_15min_hubs_2024Q3.csv'\n",
    "RTLMP2024Q3 = pd.read_csv(LMP2024Q3, skiprows=3)\n",
    "\n",
    "# 2023 Data\n",
    "LMP2023Q1 = f'{base_url}ercot_lmp_rt_15min_hubs_2023Q1.csv'\n",
    "RTLMP2023Q1 = pd.read_csv(LMP2023Q1, skiprows=3)\n",
    "\n",
    "LMP2023Q2 = f'{base_url}ercot_lmp_rt_15min_hubs_2023Q2.csv'\n",
    "RTLMP2023Q2 = pd.read_csv(LMP2023Q2, skiprows=3)\n",
    "\n",
    "LMP2023Q3 = f'{base_url}ercot_lmp_rt_15min_hubs_2023Q3.csv'\n",
    "RTLMP2023Q3 = pd.read_csv(LMP2023Q3, skiprows=3)\n",
    "\n",
    "LMP2023Q4 = f'{base_url}ercot_lmp_rt_15min_hubs_2023Q4.csv'\n",
    "RTLMP2023Q4 = pd.read_csv(LMP2023Q4, skiprows=3)\n",
    "\n",
    "# 2022 Data\n",
    "LMP2022Q1 = f'{base_url}ercot_lmp_rt_15min_hubs_2022Q1.csv'\n",
    "RTLMP2022Q1 = pd.read_csv(LMP2022Q1, skiprows=3)\n",
    "\n",
    "LMP2022Q2 = f'{base_url}ercot_lmp_rt_15min_hubs_2022Q2.csv'\n",
    "RTLMP2022Q2 = pd.read_csv(LMP2022Q2, skiprows=3)\n",
    "\n",
    "LMP2022Q3 = f'{base_url}ercot_lmp_rt_15min_hubs_2022Q3.csv'\n",
    "RTLMP2022Q3 = pd.read_csv(LMP2022Q3, skiprows=3)\n",
    "\n",
    "LMP2022Q4 = f'{base_url}ercot_lmp_rt_15min_hubs_2022Q4.csv'\n",
    "RTLMP2022Q4 = pd.read_csv(LMP2022Q4, skiprows=3)\n",
    "\n",
    "# 2021 Data\n",
    "LMP2021Q1 = f'{base_url}ercot_lmp_rt_15min_hubs_2021Q1.csv'\n",
    "RTLMP2021Q1 = pd.read_csv(LMP2021Q1, skiprows=3)\n",
    "\n",
    "LMP2021Q2 = f'{base_url}ercot_lmp_rt_15min_hubs_2021Q2.csv'\n",
    "RTLMP2021Q2 = pd.read_csv(LMP2021Q2, skiprows=3)\n",
    "\n",
    "LMP2021Q3 = f'{base_url}ercot_lmp_rt_15min_hubs_2021Q3.csv'\n",
    "RTLMP2021Q3 = pd.read_csv(LMP2021Q3, skiprows=3)\n",
    "\n",
    "LMP2021Q4 = f'{base_url}ercot_lmp_rt_15min_hubs_2021Q4.csv'\n",
    "RTLMP2021Q4 = pd.read_csv(LMP2021Q4, skiprows=3)\n",
    "\n",
    "# 2020 Data\n",
    "LMP2020Q1 = f'{base_url}ercot_lmp_rt_15min_hubs_2020Q1.csv'\n",
    "RTLMP2020Q1 = pd.read_csv(LMP2020Q1, skiprows=3)\n",
    "\n",
    "LMP2020Q2 = f'{base_url}ercot_lmp_rt_15min_hubs_2020Q2.csv'\n",
    "RTLMP2020Q2 = pd.read_csv(LMP2020Q2, skiprows=3)\n",
    "\n",
    "LMP2020Q3 = f'{base_url}ercot_lmp_rt_15min_hubs_2020Q3.csv'\n",
    "RTLMP2020Q3 = pd.read_csv(LMP2020Q3, skiprows=3)\n",
    "\n",
    "LMP2020Q4 = f'{base_url}ercot_lmp_rt_15min_hubs_2020Q4.csv'\n",
    "RTLMP2020Q4 = pd.read_csv(LMP2020Q4, skiprows=3)\n",
    "\n",
    "print(RTLMP2020Q1.shape)\n",
    "print(RTLMP2020Q2.shape)\n",
    "print(RTLMP2020Q3.shape)\n",
    "print(RTLMP2020Q4.shape)\n",
    "print(RTLMP2021Q1.shape)\n",
    "print(RTLMP2021Q2.shape)\n",
    "print(RTLMP2021Q3.shape)\n",
    "print(RTLMP2021Q4.shape)\n",
    "print(RTLMP2022Q1.shape)\n",
    "print(RTLMP2022Q2.shape)\n",
    "print(RTLMP2022Q3.shape)\n",
    "print(RTLMP2022Q4.shape)\n",
    "print(RTLMP2023Q1.shape)\n",
    "print(RTLMP2023Q2.shape)\n",
    "print(RTLMP2023Q3.shape)\n",
    "print(RTLMP2023Q4.shape)\n",
    "print(RTLMP2024Q1.shape)\n",
    "print(RTLMP2024Q2.shape)\n",
    "print(RTLMP2024Q3.shape)\n",
    "\n",
    "lables = [RTLMP2020Q1,RTLMP2020Q2,RTLMP2020Q3,RTLMP2020Q4,RTLMP2021Q1,RTLMP2021Q2,RTLMP2021Q3,RTLMP2021Q4,RTLMP2022Q1,RTLMP2022Q2,RTLMP2022Q3,RTLMP2022Q4,RTLMP2023Q1,RTLMP2023Q2,RTLMP2023Q3,RTLMP2023Q4,RTLMP2024Q1,RTLMP2024Q2,RTLMP2024Q3]\n",
    "RTLMPdata = pd.DataFrame()\n",
    "columns_to_extract = ['Local Timestamp Central Time (Interval Ending)','Bus average LMP', 'Houston LMP', 'Hub average LMP', 'North LMP', 'Panhandle LMP', 'South LMP', 'West LMP'\n",
    "]\n",
    "for lable in lables:\n",
    "    selected_columns = lable[columns_to_extract]\n",
    "    RTLMPdata = pd.concat([RTLMPdata, selected_columns], ignore_index=True)\n",
    "\n",
    "RTLMPdata.sort_values(by='Local Timestamp Central Time (Interval Ending)', inplace=True)\n",
    "\n",
    "RTLMPdata['Local Timestamp Central Time (Interval Ending)'] = pd.to_datetime(RTLMPdata['Local Timestamp Central Time (Interval Ending)'])\n",
    "\n",
    "RTLMPdata['Local Timestamp Central Time (Interval Ending)'] = RTLMPdata['Local Timestamp Central Time (Interval Ending)'].dt.ceil('H')\n",
    "\n",
    "RTLMP_Hourly_Data = RTLMPdata.groupby('Local Timestamp Central Time (Interval Ending)').agg({\n",
    "    'Bus average LMP': 'mean',\n",
    "    'Houston LMP': 'mean',\n",
    "    'Hub average LMP': 'mean',\n",
    "    'North LMP': 'mean',\n",
    "    'Panhandle LMP': 'mean',\n",
    "    'South LMP': 'mean',\n",
    "    'West LMP': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "RTLMP_Hourly_Data.head()\n",
    "\n",
    "\n",
    "## ERCOT Hourly Actual Load (megawatt) by Weather Zone (2020-2024)\n",
    "\n",
    "# 2024 Data\n",
    "lablefile2024 = f'{base_url}ercot_load_act_hr_2024.csv'\n",
    "lable2024 = pd.read_csv(lablefile2024, skiprows=3)\n",
    "\n",
    "# 2023 Data\n",
    "lablefile2023 = f'{base_url}ercot_load_act_hr_2023.csv'\n",
    "lable2023 = pd.read_csv(lablefile2023, skiprows=3)\n",
    "\n",
    "# 2022 Data\n",
    "lablefile2022 = f'{base_url}ercot_load_act_hr_2022.csv'\n",
    "lable2022 = pd.read_csv(lablefile2022, skiprows=3)\n",
    "\n",
    "# 2021 Data\n",
    "lablefile2021 = f'{base_url}ercot_load_act_hr_2021.csv'\n",
    "lable2021 = pd.read_csv(lablefile2021, skiprows=3)\n",
    "\n",
    "# 2020 Data\n",
    "lablefile2020 = f'{base_url}ercot_load_act_hr_2020.csv'\n",
    "lable2020 = pd.read_csv(lablefile2020, skiprows=3)\n",
    "\n",
    "# Renaming columns for 2020 data\n",
    "lable2020.rename(columns={'TOT Actual Load (MW)': 'TOTAL Actual Load (MW)'}, inplace=True)\n",
    "lable2020.rename(columns={'NCENT Actual Load (MW)': 'North Central Actual Load (MW)'}, inplace=True)\n",
    "lable2020.rename(columns={'FWEST Actual Load (MW)': 'Far West Actual Load (MW)'}, inplace=True)\n",
    "lable2020.rename(columns={'SCENT Actual Load (MW)': 'South Central Actual Load (MW)'}, inplace=True)\n",
    "lable2020.rename(columns={'South Actual Load (MW)': 'Southern Actual Load (MW)'}, inplace=True)\n",
    "\n",
    "# Displaying the shape of each dataset\n",
    "print(lable2020.shape)\n",
    "print(lable2021.shape)\n",
    "print(lable2022.shape)\n",
    "print(lable2023.shape)\n",
    "print(lable2024.shape)\n",
    "\n",
    "# Combining the datasets into a list\n",
    "lables = [lable2020, lable2021, lable2022, lable2023, lable2024]\n",
    "\n",
    "# Creating an empty DataFrame for combined data\n",
    "Act_Load_Data = pd.DataFrame()\n",
    "\n",
    "# Columns to extract from each dataset\n",
    "columns_to_extract = [\n",
    "    'Local Timestamp Central Time (Interval Ending)', \n",
    "    'TOTAL Actual Load (MW)', \n",
    "    'Coast Actual Load (MW)', \n",
    "    'East Actual Load (MW)', \n",
    "    'Far West Actual Load (MW)', \n",
    "    'North Actual Load (MW)', \n",
    "    'North Central Actual Load (MW)', \n",
    "    'Southern Actual Load (MW)', \n",
    "    'South Central Actual Load (MW)', \n",
    "    'West Actual Load (MW)'\n",
    "]\n",
    "\n",
    "# Concatenating the datasets and selecting the necessary columns\n",
    "for lable in lables:\n",
    "    selected_columns = lable[columns_to_extract]\n",
    "    Act_Load_Data = pd.concat([Act_Load_Data, selected_columns], ignore_index=True)\n",
    "\n",
    "# Sorting the data by the timestamp\n",
    "Act_Load_Data.sort_values(by='Local Timestamp Central Time (Interval Ending)', inplace=True)\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "Act_Load_Data.head()\n",
    "\n",
    "## ERCOT Hourly Day-Ahead Locational Marginal Prices ($/megawatt) for Hubs (2020-2024)\n",
    "\n",
    "# 2024 Data\n",
    "lmpda2024 = f'{base_url}ercot_lmp_da_hr_hubs_2024.csv'\n",
    "DALMP2024 = pd.read_csv(lmpda2024, skiprows=3)\n",
    "\n",
    "# 2023 Data\n",
    "lmpda2023 = f'{base_url}ercot_lmp_da_hr_hubs_2023.csv'\n",
    "DALMP2023 = pd.read_csv(lmpda2023, skiprows=3)\n",
    "\n",
    "# 2022 Data\n",
    "lmpda2022 = f'{base_url}ercot_lmp_da_hr_hubs_2022.csv'\n",
    "DALMP2022 = pd.read_csv(lmpda2022, skiprows=3)\n",
    "\n",
    "# 2021 Data\n",
    "lmpda2021 = f'{base_url}ercot_lmp_da_hr_hubs_2021.csv'\n",
    "DALMP2021 = pd.read_csv(lmpda2021, skiprows=3)\n",
    "\n",
    "# 2020 Data\n",
    "lmpda2020 = f'{base_url}ercot_lmp_da_hr_hubs_2020.csv'\n",
    "DALMP2020 = pd.read_csv(lmpda2020, skiprows=3)\n",
    "\n",
    "# Displaying the shape of each dataset\n",
    "print(DALMP2020.shape)\n",
    "print(DALMP2021.shape)\n",
    "print(DALMP2022.shape)\n",
    "print(DALMP2023.shape)\n",
    "print(DALMP2024.shape)\n",
    "\n",
    "# Combining the datasets into a list\n",
    "lables = [DALMP2020, DALMP2021, DALMP2022, DALMP2023, DALMP2024]\n",
    "\n",
    "# Creating an empty DataFrame for combined data\n",
    "DA_LMP_Data = pd.DataFrame()\n",
    "\n",
    "# Columns to extract from each dataset\n",
    "columns_to_extract = [\n",
    "    'Local Timestamp Central Time (Interval Ending)', \n",
    "    'Bus average LMP', \n",
    "    'Houston LMP', \n",
    "    'Hub average LMP', \n",
    "    'North LMP', \n",
    "    'Panhandle LMP', \n",
    "    'South LMP', \n",
    "    'West LMP'\n",
    "]\n",
    "\n",
    "# Concatenating the datasets and selecting the necessary columns\n",
    "for lable in lables:\n",
    "    selected_columns = lable[columns_to_extract]\n",
    "    DA_LMP_Data = pd.concat([DA_LMP_Data, selected_columns], ignore_index=True)\n",
    "\n",
    "# Sorting the data by the timestamp\n",
    "DA_LMP_Data.sort_values(by='Local Timestamp Central Time (Interval Ending)', inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the combined dataset\n",
    "DA_LMP_Data.head(10)\n",
    "\n",
    "## ERCOT Hourly Forecast Load (megawatt) by Region (2021-2024)\n",
    "\n",
    "# 2024 Data\n",
    "forecastload2024 = f'{base_url}ercot_load_for_hr_2024.csv'\n",
    "FCLoad2024 = pd.read_csv(forecastload2024, skiprows=3)\n",
    "\n",
    "# 2023 Data\n",
    "forecastload2023 = f'{base_url}ercot_load_for_hr_2023.csv'\n",
    "FCLoad2023 = pd.read_csv(forecastload2023, skiprows=3)\n",
    "\n",
    "# 2022 Data\n",
    "forecastload2022 = f'{base_url}ercot_load_for_hr_2022.csv'\n",
    "FCLoad2022 = pd.read_csv(forecastload2022, skiprows=3)\n",
    "\n",
    "# 2021 Data\n",
    "forecastload2021 = f'{base_url}ercot_load_for_hr_2021.csv'\n",
    "FCLoad2021 = pd.read_csv(forecastload2021, skiprows=3)\n",
    "\n",
    "# Displaying the shape of each dataset\n",
    "print(FCLoad2021.shape)\n",
    "print(FCLoad2022.shape)\n",
    "print(FCLoad2023.shape)\n",
    "print(FCLoad2024.shape)\n",
    "\n",
    "# Combining the datasets into a list\n",
    "lables = [FCLoad2021, FCLoad2022, FCLoad2023, FCLoad2024]\n",
    "\n",
    "# Creating an empty DataFrame for combined data\n",
    "Forecast_Load_Data = pd.DataFrame()\n",
    "\n",
    "# Columns to extract from each dataset\n",
    "columns_to_extract = [\n",
    "    'Local Timestamp Central Time (Interval Ending)', \n",
    "    'SystemTotal Forecast Load (MW)', \n",
    "    'Houston Forecast Load (MW)', \n",
    "    'North Forecast Load (MW)', \n",
    "    'South Forecast Load (MW)', \n",
    "    'West Forecast Load (MW)'\n",
    "]\n",
    "\n",
    "# Concatenating the datasets and selecting the necessary columns\n",
    "for lable in lables:\n",
    "    selected_columns = lable[columns_to_extract]\n",
    "    Forecast_Load_Data = pd.concat([Forecast_Load_Data, selected_columns], ignore_index=True)\n",
    "\n",
    "# Sorting the data by the timestamp\n",
    "Forecast_Load_Data.sort_values(by='Local Timestamp Central Time (Interval Ending)', inplace=True)\n",
    "\n",
    "# Display the first 10 rows of the combined dataset\n",
    "Forecast_Load_Data.head(10)\n",
    "\n",
    "## ERCOT Hourly Imports and Exports (megawatt) by DC Tie Line (2021-2024)\n",
    "\n",
    "# 2024 Data\n",
    "DC2024 = f'{base_url}ercot_int_all_hr_2024.csv'\n",
    "Ex_Im_Load_2024 = pd.read_csv(DC2024, skiprows=3)\n",
    "\n",
    "# 2023 Data\n",
    "DC2023 = f'{base_url}ercot_int_all_hr_2023.csv'\n",
    "Ex_Im_Load_2023 = pd.read_csv(DC2023, skiprows=3)\n",
    "\n",
    "# 2022 Data\n",
    "DC2022 = f'{base_url}ercot_int_all_hr_2022.csv'\n",
    "Ex_Im_Load_2022 = pd.read_csv(DC2022, skiprows=3)\n",
    "\n",
    "# 2021 Data\n",
    "DC2021 = f'{base_url}ercot_int_all_hr_2021.csv'\n",
    "Ex_Im_Load_2021 = pd.read_csv(DC2021, skiprows=3)\n",
    "\n",
    "# Displaying the shape of each dataset\n",
    "print(Ex_Im_Load_2021.shape)\n",
    "print(Ex_Im_Load_2022.shape)\n",
    "print(Ex_Im_Load_2023.shape)\n",
    "print(Ex_Im_Load_2024.shape)\n",
    "\n",
    "# Displaying the columns of each dataset\n",
    "print(Ex_Im_Load_2021.columns)\n",
    "print(Ex_Im_Load_2022.columns)\n",
    "print(Ex_Im_Load_2023.columns)\n",
    "print(Ex_Im_Load_2024.columns)\n",
    "\n",
    "# Combining the datasets into a list\n",
    "lables = [Ex_Im_Load_2021, Ex_Im_Load_2022, Ex_Im_Load_2023, Ex_Im_Load_2024]\n",
    "\n",
    "# Creating an empty DataFrame for combined data\n",
    "Ex_Im_Data = pd.DataFrame()\n",
    "\n",
    "# Columns to extract from each dataset\n",
    "columns_to_extract = [\n",
    "    'Local Timestamp Central Time (Interval Ending)',\n",
    "    'Comision Federal de Electricidad, Laredo Net Export (MW)',\n",
    "    'Comision Federal de Electricidad, Railroad Net Export (MW)',\n",
    "    'Southwest Power Pool, East Net Export (MW)',\n",
    "    'Southwest Power Pool, North Net Export (MW)'\n",
    "]\n",
    "\n",
    "# Concatenating the datasets and selecting the necessary columns\n",
    "for lable in lables:\n",
    "    selected_columns = lable[columns_to_extract]\n",
    "    Ex_Im_Data = pd.concat([Ex_Im_Data, selected_columns], ignore_index=True)\n",
    "\n",
    "# Sorting the data by the timestamp\n",
    "Ex_Im_Data.sort_values(by='Local Timestamp Central Time (Interval Ending)', inplace=True)\n",
    "\n",
    "# Converting the timestamp to datetime format and rounding up to the nearest hour\n",
    "Ex_Im_Data['Local Timestamp Central Time (Interval Ending)'] = pd.to_datetime(Ex_Im_Data['Local Timestamp Central Time (Interval Ending)'])\n",
    "Ex_Im_Data['Local Timestamp Central Time (Interval Ending)'] = Ex_Im_Data['Local Timestamp Central Time (Interval Ending)'].dt.ceil('H')\n",
    "\n",
    "# Display the first 10 rows of the combined dataset\n",
    "Ex_Im_Data.head(10)\n",
    "\n",
    "## ERCOT Hourly Temperatures (Fahrenheit) in Major Cities (2020-2024)\n",
    "\n",
    "# 2024 Data\n",
    "tempfile2024 = f'{base_url}ercot_temp_hr_2024.csv'\n",
    "temp2024 = pd.read_csv(tempfile2024, skiprows=3)\n",
    "\n",
    "# 2023 Data\n",
    "tempfile2023 = f'{base_url}ercot_temp_hr_2023.csv'\n",
    "temp2023 = pd.read_csv(tempfile2023, skiprows=3)\n",
    "\n",
    "# 2022 Data\n",
    "tempfile2022 = f'{base_url}ercot_temp_hr_2022.csv'\n",
    "temp2022 = pd.read_csv(tempfile2022, skiprows=3)\n",
    "\n",
    "# 2021 Data\n",
    "tempfile2021 = f'{base_url}ercot_temp_hr_2021.csv'\n",
    "temp2021 = pd.read_csv(tempfile2021, skiprows=3)\n",
    "\n",
    "# 2020 Data\n",
    "tempfile2020 = f'{base_url}ercot_temp_hr_2020.csv'\n",
    "temp2020 = pd.read_csv(tempfile2020, skiprows=3)\n",
    "\n",
    "# Displaying the shape of each dataset\n",
    "print(temp2020.shape)\n",
    "print(temp2021.shape)\n",
    "print(temp2022.shape)\n",
    "print(temp2023.shape)\n",
    "print(temp2024.shape)\n",
    "\n",
    "# Combine the datasets into a list\n",
    "temps = [temp2020, temp2021, temp2022, temp2023, temp2024]\n",
    "\n",
    "# Create an empty DataFrame to store the combined data\n",
    "all_years_temp = pd.DataFrame()\n",
    "\n",
    "# Process each year's data and calculate the average temperature\n",
    "for i, temp in enumerate(temps):\n",
    "    temps[i]['Average Temperature (Fahrenheit)'] = (\n",
    "        temps[i]['Dallas / Fort Worth Temperature (Fahrenheit)'] + \n",
    "        temps[i]['Houston International Airport Temperature (Fahrenheit)']\n",
    "    ) / 2\n",
    "    \n",
    "    # Select columns for each dataset\n",
    "    temp_selected_columns = [\n",
    "        'Local Timestamp Central Time (Interval Ending)',\n",
    "        'Average Temperature (Fahrenheit)',\n",
    "        'Dallas / Fort Worth Temperature (Fahrenheit)',\n",
    "        'Dallas / Fort Worth Temperature Observation Time (Central)',\n",
    "        'Houston International Airport Temperature (Fahrenheit)',\n",
    "        'Houston International Airport Temperature Observation Time (Central)'\n",
    "    ]\n",
    "    \n",
    "    selected_columns = temps[i][temp_selected_columns]\n",
    "    all_years_temp = pd.concat([all_years_temp, selected_columns], ignore_index=True)\n",
    "\n",
    "# Sort the data by the timestamp\n",
    "all_years_temp.sort_values(by='Local Timestamp Central Time (Interval Ending)', inplace=True)\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "all_years_temp.head()\n",
    "\n",
    "# Standardize the timestamp column in all dataframes\n",
    "dataframes = [FuelMix_Hourly_Data, RTLMP_Hourly_Data, Act_Load_Data, DA_LMP_Data, Forecast_Load_Data, Ex_Im_Data, all_years_temp]\n",
    "for df in dataframes:\n",
    "    if 'Local Timestamp Central Time (Interval Ending)' in df.columns:\n",
    "        df['Local Timestamp Central Time (Interval Ending)'] = pd.to_datetime(\n",
    "            df['Local Timestamp Central Time (Interval Ending)'], errors='coerce')\n",
    "\n",
    "# Start merging with the first dataframe and merge the rest one by one\n",
    "merged_df = dataframes[0]\n",
    "\n",
    "for df in dataframes[1:]:\n",
    "    if 'Local Timestamp Central Time (Interval Ending)' in df.columns:\n",
    "        merged_df = pd.merge(\n",
    "            merged_df, \n",
    "            df, \n",
    "            on='Local Timestamp Central Time (Interval Ending)', \n",
    "            how='inner')  # 'inner' ensures only common timestamps are kept\n",
    "\n",
    "# Display the first 10 rows of the merged dataframe\n",
    "merged_df.head(10)\n",
    "\n",
    "\n",
    "merged_df.shape\n",
    "\n",
    "\n",
    "merged_df.columns\n",
    "# Create lag1 data for actual load\n",
    "### FIXED: changed from shift(-1) to shift(1) so the data at hour n moves to hour n+1\n",
    "merged_df['TOTAL Actual Load (MW)_lag1'] = merged_df['TOTAL Actual Load (MW)'].shift(1)\n",
    "merged_df.dropna(subset=['TOTAL Actual Load (MW)_lag1'], inplace=True)\n",
    "print(merged_df[['Local Timestamp Central Time (Interval Ending)',\n",
    "       'UTC Timestamp (Interval Ending)',\n",
    "       'Local Timestamp Central Time (Interval Beginning)', 'Local Date',\n",
    "       'Hour Number','TOTAL Actual Load (MW)', 'TOTAL Actual Load (MW)_lag1']].head())\n",
    "\n",
    "merged_df['Year'] = merged_df['Local Timestamp Central Time (Interval Ending)'].dt.year\n",
    "merged_df['Month'] = merged_df['Local Timestamp Central Time (Interval Ending)'].dt.month\n",
    "merged_df['Day'] = merged_df['Local Timestamp Central Time (Interval Ending)'].dt.day\n",
    "merged_df['Hour'] = merged_df['Local Timestamp Central Time (Interval Ending)'].dt.hour\n",
    "merged_df['DayOfWeek'] = merged_df['Local Timestamp Central Time (Interval Ending)'].dt.dayofweek\n",
    "merged_df['DayOfYear'] = merged_df['Local Timestamp Central Time (Interval Ending)'].dt.dayofyear\n",
    "merged_df['IsWeekend'] = merged_df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "merged_df['Month_sin'] = np.sin(2 * np.pi * merged_df['Month'] / 12)\n",
    "merged_df['Month_cos'] = np.cos(2 * np.pi * merged_df['Month'] / 12)\n",
    "\n",
    "merged_df['Hour_sin'] = np.sin(2 * np.pi * merged_df['Hour'] / 24)\n",
    "merged_df['Hour_cos'] = np.cos(2 * np.pi * merged_df['Hour'] / 24)\n",
    "\n",
    "merged_df['DayOfWeek_sin'] = np.sin(2 * np.pi * merged_df['DayOfWeek'] / 7)\n",
    "merged_df['DayOfWeek_cos'] = np.cos(2 * np.pi * merged_df['DayOfWeek'] / 7)\n",
    "\n",
    "merged_df.columns\n",
    "# Check if Panhandle LMP exists\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Combine regions\n",
    "merged_df['North Load (MW)'] = merged_df[['North Actual Load (MW)', 'North Central Actual Load (MW)']].mean(axis=1)\n",
    "merged_df['South Load (MW)'] = merged_df[['Southern Actual Load (MW)', 'South Central Actual Load (MW)']].mean(axis=1)\n",
    "merged_df['East Load (MW)'] = merged_df['East Actual Load (MW)']\n",
    "merged_df['West Load (MW)'] = merged_df[['Far West Actual Load (MW)', 'West Actual Load (MW)']].mean(axis=1)\n",
    "merged_df['Coast Load (MW)'] = merged_df['Coast Actual Load (MW)']\n",
    "\n",
    "\n",
    "# Combine temperature for North and South regions\n",
    "merged_df['North Temp (F)'] = merged_df['Dallas / Fort Worth Temperature (Fahrenheit)']\n",
    "merged_df['South Temp (F)'] = merged_df['Houston International Airport Temperature (Fahrenheit)']\n",
    "\n",
    "# Combine LMPs into the same 5 regions used for load\n",
    "\n",
    "# North region LMP (combining North LMP and North Central LMP)\n",
    "merged_df['North LMP'] = merged_df[['North LMP_x', 'North LMP_y', 'Panhandle LMP_x', 'Panhandle LMP_y']].mean(axis=1)\n",
    "\n",
    "# South region LMP (combining South LMP and South Central LMP)\n",
    "merged_df['South LMP'] = merged_df[['South LMP_x', 'South LMP_y', 'Houston LMP_x', 'Houston LMP_y']].mean(axis=1)\n",
    "\n",
    "# West region LMP (combining West LMP and Far West LMP)\n",
    "merged_df['West LMP'] = merged_df[['West LMP_x', 'West LMP_y']].mean(axis=1)\n",
    "\n",
    "# # East region LMP (combining East and Coast)\n",
    "# merged_df['East LMP'] = merged_df[['Hub average LMP_x', 'Hub average LMP_y']].mean(axis=1)\n",
    "\n",
    "# # Coast region LMP (same as East but kept separately if needed)\n",
    "# merged_df['Coast LMP'] = merged_df[['Hub average LMP_x', 'Hub average LMP_y']].mean(axis=1)\n",
    "\n",
    "# Display the first few rows after combining the LMPs\n",
    "merged_df.head(10)\n",
    "\n",
    "\n",
    "# Display the first few rows after combining\n",
    "merged_df.head(10)\n",
    "# # Define columns to drop after the combination of regions\n",
    "# columns_to_drop_after_combination = [\n",
    "#     'North Actual Load (MW)', 'North Central Actual Load (MW)', 'South Actual Load (MW)',\n",
    "#     'South Central Actual Load (MW)', 'West Actual Load (MW)', 'Far West Actual Load (MW)',\n",
    "#     'East Actual Load (MW)', 'Coast Actual Load (MW)', 'Panhandle LMP', 'Bus average LMP_x',\n",
    "#     'Bus average LMP_y', 'Houston LMP_x', 'Houston LMP_y', 'Hub average LMP_x', 'Hub average LMP_y',\n",
    "#     'North LMP_x', 'North LMP_y', 'South LMP_x', 'South LMP_y', 'West LMP_x', 'West LMP_y', 'Panhandle LMP_x', 'Panhandle LMP_y'\n",
    "#     'Dallas / Fort Worth Temperature (Fahrenheit)', \n",
    "#     'Houston International Airport Temperature (Fahrenheit)', \n",
    "#     'Houston International Airport Temperature Observation Time (Central)',\n",
    "#     'Dallas / Fort Worth Temperature Observation Time (Central)'\n",
    "# ]\n",
    "\n",
    "# # Drop the unnecessary columns from the dataframe, ignoring any missing columns\n",
    "# merged_df = merged_df.drop(columns=columns_to_drop_after_combination, errors='ignore')\n",
    "\n",
    "# # Display the remaining columns after dropping\n",
    "# merged_df.head(10)\n",
    "\n",
    "# Create lag1 data for LMP\n",
    "### FIXED: changed from shift(-1) to shift(1) so the data at hour n moves to hour n+1\n",
    "merged_df['North_LMP_lag1'] = merged_df['North LMP'].shift(1)\n",
    "merged_df.dropna(subset=['North_LMP_lag1'], inplace=True)\n",
    "\n",
    "merged_df['South_LMP_lag1'] = merged_df['South LMP'].shift(1)\n",
    "merged_df.dropna(subset=['South_LMP_lag1'], inplace=True)\n",
    "\n",
    "merged_df['West_LMP_lag1'] = merged_df['West LMP'].shift(1)\n",
    "merged_df.dropna(subset=['West_LMP_lag1'], inplace=True)\n",
    "# Define columns to drop after the combination of regions\n",
    "columns_to_drop_after_combination = [\n",
    "    'North Actual Load (MW)', 'North Central Actual Load (MW)', 'South Actual Load (MW)',\n",
    "    'South Central Actual Load (MW)', 'West Actual Load (MW)', 'Far West Actual Load (MW)',\n",
    "    'East Actual Load (MW)', 'Coast Actual Load (MW)', 'Panhandle LMP', 'Bus average LMP_x',\n",
    "    'Bus average LMP_y', 'Houston LMP_x', 'Houston LMP_y', 'Hub average LMP_x', 'Hub average LMP_y',\n",
    "    'North LMP_x', 'North LMP_y', 'South LMP_x', 'South LMP_y', 'West LMP_x', 'West LMP_y', 'Panhandle LMP_x', 'Panhandle LMP_y'\n",
    "    'Dallas / Fort Worth Temperature (Fahrenheit)', \n",
    "    'Houston International Airport Temperature (Fahrenheit)', \n",
    "    'Houston International Airport Temperature Observation Time (Central)',\n",
    "    'Dallas / Fort Worth Temperature Observation Time (Central)'\n",
    "]\n",
    "\n",
    "# Drop the unnecessary columns from the dataframe, ignoring any missing columns\n",
    "merged_df = merged_df.drop(columns=columns_to_drop_after_combination, errors='ignore')\n",
    "\n",
    "# Display the remaining columns after dropping\n",
    "merged_df.head(10)\n",
    "# Check if the columns you're trying to merge exist\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Select only the numeric columns for correlation matrix\n",
    "numeric_df = merged_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Plotting the correlation matrix\n",
    "plt.figure(figsize=(20, 18))\n",
    "corr_matrix = numeric_df.corr()  # Use the dataframe with only numeric columns\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "# Remove non-numeric columns for correlation calculation\n",
    "numeric_columns = merged_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculate correlation with the numeric data only\n",
    "corr_with_target = merged_df[numeric_columns].corr()['TOTAL Actual Load (MW)']\n",
    "corr_with_target_sorted = corr_with_target.sort_values(ascending=False)\n",
    "print(corr_with_target_sorted)\n",
    "\n",
    "high_corr_columns = corr_matrix['TOTAL Actual Load (MW)_lag1'][(corr_matrix['TOTAL Actual Load (MW)_lag1'] > 0.5) | (corr_matrix['TOTAL Actual Load (MW)_lag1'] < -0.5)]\n",
    "\n",
    "high_corr_columns\n",
    "features = [ \n",
    "       'Biomass Generation (MW)',\n",
    "       'Coal Generation (MW)', \n",
    "       'Gas Generation (MW)', \n",
    "       'Gas-CC Generation (MW)',\n",
    "       'Solar Generation (MW)',\n",
    "       'Average Temperature (Fahrenheit)']\n",
    "X=merged_df[features].copy()\n",
    "\n",
    "y=merged_df['TOTAL Actual Load (MW)_lag1']\n",
    "\n",
    "nan_count = np.sum(X.isnull(), axis = 0)\n",
    "nan_count\n",
    "X['Average Temperature (Fahrenheit)'].fillna(value = X['Average Temperature (Fahrenheit)'].mean(), inplace = True)\n",
    "nan_count = np.sum(X.isnull(), axis = 0)\n",
    "nan_count\n",
    "nan_count_y = np.sum(y.isnull(), axis = 0)\n",
    "nan_count_y\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(features), figsize=(15, 5))\n",
    "\n",
    "for ax, feature in zip(axes, features):\n",
    "    ax.boxplot(X[feature])\n",
    "    ax.set_title(f'{feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "import scipy.stats as stats\n",
    "X['Gas Generation (MW)'] = stats.mstats.winsorize(X['Gas Generation (MW)'], limits=[0.06, 0.06])\n",
    "X['Solar Generation (MW)'] = stats.mstats.winsorize(X['Solar Generation (MW)'], limits=[0.05, 0.05])\n",
    "X['Average Temperature (Fahrenheit)'] = stats.mstats.winsorize(X['Average Temperature (Fahrenheit)'], limits=[0.05, 0.05])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(features), figsize=(15, 5))\n",
    "\n",
    "for ax, feature in zip(axes, features):\n",
    "    ax.boxplot(X[feature])\n",
    "    ax.set_title(f'{feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_regressor = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_regressor.predict(X_test)\n",
    "\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(best_regressor)\n",
    "print(f\"Best Model MAE: {mae_best}\")\n",
    "print(f\"Best Model MSE: {mse_best}\")\n",
    "print(f\"Best Model R-squared (R²): {r2_best}\")\n",
    "model = DecisionTreeRegressor(max_depth=5, min_samples_leaf=4, min_samples_split=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fi = model.feature_importances_\n",
    "\n",
    "names_sorted = [x for _, x in sorted(zip(fi, X_train.columns.values), reverse=True)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(np.arange(5), sorted(fi, reverse=True)[:5], width=0.35)\n",
    "ax.set_xticks(np.arange(5))\n",
    "ax.set_xticklabels(names_sorted[:5], rotation=90)\n",
    "plt.title('Top 5 Feature Importances from Decision Tree')\n",
    "ax.set_ylabel('Normalized Importance')\n",
    "\n",
    "plt.show()\n",
    "# # Check if Panhandle LMP exists\n",
    "# print(merged_df.columns)\n",
    "\n",
    "# # Combine regions\n",
    "# merged_df['North Load (MW)'] = merged_df[['North Actual Load (MW)', 'North Central Actual Load (MW)']].mean(axis=1)\n",
    "# merged_df['South Load (MW)'] = merged_df[['Southern Actual Load (MW)', 'South Central Actual Load (MW)']].mean(axis=1)\n",
    "# merged_df['East Load (MW)'] = merged_df['East Actual Load (MW)']\n",
    "# merged_df['West Load (MW)'] = merged_df[['Far West Actual Load (MW)', 'West Actual Load (MW)']].mean(axis=1)\n",
    "# merged_df['Coast Load (MW)'] = merged_df['Coast Actual Load (MW)']\n",
    "\n",
    "\n",
    "# # Combine temperature for North and South regions\n",
    "# merged_df['North Temp (F)'] = merged_df['Dallas / Fort Worth Temperature (Fahrenheit)']\n",
    "# merged_df['South Temp (F)'] = merged_df['Houston International Airport Temperature (Fahrenheit)']\n",
    "\n",
    "# # Combine LMPs into the same 5 regions used for load\n",
    "\n",
    "# # North region LMP (combining North LMP and North Central LMP)\n",
    "# merged_df['North LMP'] = merged_df[['North LMP_x', 'North LMP_y', 'Panhandle LMP_x', 'Panhandle LMP_y']].mean(axis=1)\n",
    "\n",
    "# # South region LMP (combining South LMP and South Central LMP)\n",
    "# merged_df['South LMP'] = merged_df[['South LMP_x', 'South LMP_y', 'Houston LMP_x', 'Houston LMP_y']].mean(axis=1)\n",
    "\n",
    "# # West region LMP (combining West LMP and Far West LMP)\n",
    "# merged_df['West LMP'] = merged_df[['West LMP_x', 'West LMP_y']].mean(axis=1)\n",
    "\n",
    "# # # East region LMP (combining East and Coast)\n",
    "# # merged_df['East LMP'] = merged_df[['Hub average LMP_x', 'Hub average LMP_y']].mean(axis=1)\n",
    "\n",
    "# # # Coast region LMP (same as East but kept separately if needed)\n",
    "# # merged_df['Coast LMP'] = merged_df[['Hub average LMP_x', 'Hub average LMP_y']].mean(axis=1)\n",
    "\n",
    "# # Display the first few rows after combining the LMPs\n",
    "# merged_df.head(10)\n",
    "\n",
    "\n",
    "# # Display the first few rows after combining\n",
    "# merged_df.head(10)\n",
    "\n",
    "\n",
    "# # Define columns to drop after the combination of regions\n",
    "# columns_to_drop_after_combination = [\n",
    "#     'North Actual Load (MW)', 'North Central Actual Load (MW)', 'South Actual Load (MW)',\n",
    "#     'South Central Actual Load (MW)', 'West Actual Load (MW)', 'Far West Actual Load (MW)',\n",
    "#     'East Actual Load (MW)', 'Coast Actual Load (MW)', 'Panhandle LMP', 'Bus average LMP_x',\n",
    "#     'Bus average LMP_y', 'Houston LMP_x', 'Houston LMP_y', 'Hub average LMP_x', 'Hub average LMP_y',\n",
    "#     'North LMP_x', 'North LMP_y', 'South LMP_x', 'South LMP_y', 'West LMP_x', 'West LMP_y', 'Panhandle LMP_x', 'Panhandle LMP_y'\n",
    "#     'Dallas / Fort Worth Temperature (Fahrenheit)', \n",
    "#     'Houston International Airport Temperature (Fahrenheit)', \n",
    "#     'Houston International Airport Temperature Observation Time (Central)',\n",
    "#     'Dallas / Fort Worth Temperature Observation Time (Central)'\n",
    "# ]\n",
    "\n",
    "# # Drop the unnecessary columns from the dataframe, ignoring any missing columns\n",
    "# merged_df = merged_df.drop(columns=columns_to_drop_after_combination, errors='ignore')\n",
    "\n",
    "# # Display the remaining columns after dropping\n",
    "# merged_df.head(10)\n",
    "\n",
    "\n",
    "# print(merged_df.columns)\n",
    "\n",
    "# Display the first few rows\n",
    "merged_df.head()\n",
    "\n",
    "# Check for missing values in the merged columns\n",
    "merged_df.isna().sum()\n",
    "\n",
    "# Fill missing temperature values with the mean of each column\n",
    "merged_df['Average Temperature (Fahrenheit)'].fillna(merged_df['Average Temperature (Fahrenheit)'].mean(), inplace=True)\n",
    "merged_df['Dallas / Fort Worth Temperature (Fahrenheit)'].fillna(merged_df['Dallas / Fort Worth Temperature (Fahrenheit)'].mean(), inplace=True)\n",
    "merged_df['North Temp (F)'].fillna(merged_df['North Temp (F)'].mean(), inplace=True)\n",
    "merged_df['South Temp (F)'].fillna(merged_df['South Temp (F)'].mean(), inplace=True)\n",
    "\n",
    "# Check if there are still missing values\n",
    "print(merged_df.isna().sum())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define your target and features\n",
    "target = merged_df['TOTAL Actual Load (MW)']  # You can change this to any other target\n",
    "features = merged_df[['North Load (MW)', 'South Load (MW)', 'East Load (MW)', 'West Load (MW)', 'Coast Load (MW)',\n",
    "                      'North LMP', 'South LMP', 'West LMP', 'East LMP', 'Coast LMP']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')\n",
    "\n",
    "\n",
    "This means that the relationship between the loads and locational marginal prices (LMPs) is almost perfectly linear."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
